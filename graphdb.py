import json
import os
import numpy as np
from llm import llm
from collections import Counter
from sentence_transformers import SentenceTransformer

DATABASE = "neo4j"

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # ë” ìž‘ì€ ëª¨ë¸ ì‚¬ìš©


def create_summary_context(trace_data):
    context_lines = []

    def get_tag_value(tags, key, default=None):
        for tag in tags:
            if tag.get("key") == key:
                return tag.get("value")
        return default

    sigma_alerts = Counter()
    process_flows = Counter()
    network_events = Counter()
    file_events = Counter()
    registry_events = Counter()

    for span in trace_data.get("spans", []):
        tags = span.get("tags", [])
        event_name = get_tag_value(tags, "EventName", "")
        process_image = get_tag_value(tags, "Image")
        process_name = os.path.basename(process_image or "N/A")

        # Sigma ë£° íƒì§€
        rule_title = get_tag_value(tags, "sigma.rule_title")
        if rule_title:
            sigma_alerts[f"ê·œì¹™: {rule_title}, í”„ë¡œì„¸ìŠ¤: {process_name}"] += 1

        # í”„ë¡œì„¸ìŠ¤ ìƒì„± íë¦„
        if "ProcessCreate" in event_name:
            parent_image = get_tag_value(tags, "ParentImage")
            if parent_image and process_image:
                process_flows[
                    f"'{os.path.basename(parent_image)}'ê°€ '{process_name}'ë¥¼ ì‹¤í–‰"
                ] += 1

        # ë„¤íŠ¸ì›Œí¬/íŒŒì¼/ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ë²¤íŠ¸
        if "NetworkConnect" in event_name:
            dest_ip = get_tag_value(tags, "DestinationIp")
            dest_port = get_tag_value(tags, "DestinationPort")
            if dest_ip and dest_port:
                network_events[
                    f"[ë„¤íŠ¸ì›Œí¬] '{process_name}'ê°€ '{dest_ip}:{dest_port}'ë¡œ ì—°ê²°"
                ] += 1
        elif "FileCreate" in event_name:
            target_file = get_tag_value(tags, "TargetFilename")
            if target_file:
                file_events[
                    f"[íŒŒì¼] '{process_name}'ê°€ '{target_file}' íŒŒì¼ì„ ìƒì„±"
                ] += 1
        elif "RegistryValueSet" in event_name:
            target_object = get_tag_value(tags, "TargetObject")
            if target_object:
                registry_events[
                    f"[ë ˆì§€ìŠ¤íŠ¸ë¦¬] '{process_name}'ê°€ '{target_object}' í‚¤ ê°’ì„ ìˆ˜ì •"
                ] += 1

    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    if sigma_alerts:
        context_lines.append("### Sigma Rule íƒì§€ ìš”ì•½:")
        for item, count in sigma_alerts.most_common():
            context_lines.append(f"- {item} ({count}íšŒ)")
    if process_flows:
        context_lines.append("\n### ì£¼ìš” í”„ë¡œì„¸ìŠ¤ ìƒì„± íë¦„:")
        for item, count in process_flows.most_common():
            context_lines.append(f"- {item} ({count}íšŒ)")
    if network_events or file_events or registry_events:
        context_lines.append("\n### ê¸°íƒ€ ì£¼ìš” ì´ë²¤íŠ¸:")
        for item, count in network_events.most_common(5):
            context_lines.append(f"- {item} ({count}íšŒ)")
        for item, count in file_events.most_common(5):
            context_lines.append(f"- {item} ({count}íšŒ)")
        for item, count in registry_events.most_common(5):
            context_lines.append(f"- {item} ({count}íšŒ)")

    return "\n".join(context_lines)


def summarize_trace_with_llm(trace_input, prompt_template):
    if isinstance(trace_input, str):
        with open(trace_input, "r", encoding="utf-8-sig") as f:
            trace_data = json.load(f)
    else:
        trace_data = trace_input

    summary_context = create_summary_context(trace_data)
    final_prompt = prompt_template.replace(
        "[ë¶„ì„í•  JSON ë°ì´í„°ê°€ ì—¬ê¸°ì— ì‚½ìž…ë©ë‹ˆë‹¤]", summary_context
    )

    try:
        response = llm.invoke(final_prompt)
        raw_content = response.content
        if not raw_content.strip():
            return {"error": "LLMìœ¼ë¡œë¶€í„° ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤."}

        cleaned_content = raw_content.strip()
        if cleaned_content.startswith("```json"):
            cleaned_content = cleaned_content.split("\n", 1)[1]
        if cleaned_content.endswith("```"):
            cleaned_content = cleaned_content.rsplit("\n", 1)[0]

        analysis_result = json.loads(cleaned_content.strip())
        return analysis_result
    except json.JSONDecodeError:
        return {
            "error": "LLMì´ ìœ íš¨í•œ JSONì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "raw_response": raw_content,
        }
    except Exception as e:
        return {"error": f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}


def cosine_similarity(vec1, vec2):
    v1 = np.array(vec1, dtype=float)
    v2 = np.array(vec2, dtype=float)
    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:
        return 0.0
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))


def find_similar_traces(driver, summary_text, top_k=5):
    with driver.session(database=DATABASE) as session:
        all_summaries = session.run(
            """
            MATCH (s:Summary)-[:SUMMARIZES]->(t:Trace)
            RETURN 
                coalesce(t.traceId, t.`traceId:ID(Trace)`) AS trace_id, 
                s.embedding AS embedding
        """
        )

        summary_embedding = embedding_model.encode(summary_text)
        similarities = []

        for record in all_summaries:
            trace_id = record["trace_id"]
            emb = record["embedding"]

            if isinstance(emb, str):
                try:
                    emb = json.loads(emb)
                except json.JSONDecodeError:
                    continue

            if emb is None:
                continue

            sim = cosine_similarity(summary_embedding, emb)
            similarities.append({"trace_id": trace_id, "similarity": sim})

        similarities.sort(key=lambda x: x["similarity"], reverse=True)
        return similarities[:top_k]


def generate_mitigation_prompt(
    summary_result, structural_similarity, indirect_connections
):
    """
    LLMì—ê²Œ ì•…ì„± í–‰ìœ„ ëŒ€ì‘ ë°©ì•ˆì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    summary_text = summary_result.get("summary", "")

    similar_entities = set()
    for s in structural_similarity:
        similar_entities.update(s["common_entities"])

    for c in indirect_connections:
        similar_entities.add(c["e1_name"])
        similar_entities.add(c["e2_name"])

    prompt = f"""
    ë‹¹ì‹ ì€ ë³´ì•ˆ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì•„ëž˜ íŠ¸ë ˆì´ìŠ¤ ë¶„ì„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ì—… í™˜ê²½ì—ì„œ ë°œê²¬ëœ ì•…ì„± í–‰ìœ„ì— ëŒ€í•œ 
    ì‹¤ì œ ëŒ€ì‘ ë°©ì•ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆí•´ì£¼ì„¸ìš”.

    [íŠ¸ë ˆì´ìŠ¤ ìš”ì•½]
    {summary_text}

    [ì—°ê´€ ì—”í‹°í‹°]
    {', '.join(similar_entities)}

    [ìš”ì²­]
    1. íƒì§€ëœ ì•…ì„± í”„ë¡œì„¸ìŠ¤ ë° íŒŒì¼ ê²©ë¦¬ ë°©ë²•
    2. ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ë° ì™¸ë¶€ í†µì‹  í†µì œ ë°©ì•ˆ
    3. ë¡œê·¸/ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ê°•í™” ë°©ë²•
    4. í–¥í›„ ìœ ì‚¬ ê³µê²© ì˜ˆë°© ì „ëžµ
    5. ì‹¤ë¬´ìžê°€ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ ëŒ€ì‘ ê¶Œìž¥

    ì‘ë‹µì€ JSON ë˜ëŠ” ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìž‘ì„±í•˜ê³ , ë‹¨ê³„ë³„ë¡œ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ìƒì„¸ížˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    ì–¸ì–´ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
    """
    return prompt


def analyze_structural_similarity_no_db(driver, new_trace, prompt_template, top_k=5):
    # LLM ìš”ì•½
    summary_result = summarize_trace_with_llm(new_trace, prompt_template)
    if "error" in summary_result:
        return summary_result
    summary_text = summary_result.get("summary", "")

    # ì˜ë¯¸ì  ìœ ì‚¬ íŠ¸ë ˆì´ìŠ¤ ê²€ìƒ‰
    top_similar_traces = find_similar_traces(driver, summary_text, top_k=top_k)
    similar_ids = [t["trace_id"] for t in top_similar_traces]

    print(f"\nðŸ” ì˜ë¯¸ì  ìœ ì‚¬ë„ ìƒìœ„ {top_k} íŠ¸ë ˆì´ìŠ¤: {similar_ids}\n")

    # êµ¬ì¡°ì  ìœ ì‚¬ì„± ë¶„ì„
    with driver.session(database=DATABASE) as session:
        res = session.run(
            """
            MATCH (s:Summary)-[:SUMMARIZES]->(t:Trace)
            WHERE t.traceId IN $trace_ids
            OPTIONAL MATCH (s)-[:USES_TECHNIQUE]->(tech)
            OPTIONAL MATCH (t)<-[:PARTICIPATED_IN]-(ent)
            RETURN 
                t.traceId AS trace_id,
                collect(DISTINCT
                    CASE labels(ent)[0]
                        WHEN 'Process' THEN ent.processName
                        WHEN 'File' THEN ent.filePath
                        WHEN 'User' THEN ent.userName
                        WHEN 'Ip' THEN ent.ipAddress
                        WHEN 'Registry' THEN ent.keyPath
                        ELSE null
                    END
                ) AS entities,
                collect(DISTINCT tech.name) AS techniques

        """,
            trace_ids=similar_ids,
        )

        trace_entities = summary_result.get("key_entities", [])
        new_entities = set(
            e["value"].strip().lower().replace("\\", "/")
            for e in trace_entities
            if isinstance(e, dict) and "value" in e
        )

        comparisons = []
        for record in res:

            db_entities = set(
                (e or "").strip().lower().replace("\\", "/")
                for e in record["entities"]
                if e and e != "-"
            )
            # ê³µí†µ ì—”í‹°í‹°
            common_entities = new_entities & db_entities

            comparisons.append(
                {
                    "trace_id": record["trace_id"],
                    "common_entities": list(common_entities),
                    "entity_match_count": len(common_entities),
                }
            )

        comparisons.sort(key=lambda x: (x["entity_match_count"]), reverse=True)

        # ê°„ì ‘ ì—°ê²° íƒìƒ‰
        with driver.session(database=DATABASE) as session:
            query = """
                UNWIND $trace_ids AS trace_id
                MATCH (s:Summary)-[:SUMMARIZES]->(t:Trace {traceId: trace_id})
                OPTIONAL MATCH (t)<-[:PARTICIPATED_IN]-(ent)
                WITH collect(DISTINCT
                    CASE labels(ent)[0]
                        WHEN 'Process' THEN ent.processName
                        WHEN 'File' THEN ent.filePath
                        WHEN 'User' THEN ent.userName
                        WHEN 'Ip' THEN ent.ipAddress
                        WHEN 'Registry' THEN ent.keyPath
                        ELSE null
                    END
                ) AS groupEntities
                UNWIND groupEntities AS e1
                UNWIND groupEntities AS e2
                WITH e1, e2 WHERE e1 IS NOT NULL AND e2 IS NOT NULL AND e1 < e2
                MATCH path = shortestPath(
                    (n1)-[*..2]-(n2)
                )
                WHERE 
                    ( (labels(n1)[0] = 'Process' AND n1.processName = e1) OR
                    (labels(n1)[0] = 'File' AND n1.filePath = e1) OR
                    (labels(n1)[0] = 'User' AND n1.userName = e1) OR
                    (labels(n1)[0] = 'Ip' AND n1.ipAddress = e1) OR
                    (labels(n1)[0] = 'Registry' AND n1.keyPath = e1) )
                AND
                    ( (labels(n2)[0] = 'Process' AND n2.processName = e2) OR
                    (labels(n2)[0] = 'File' AND n2.filePath = e2) OR
                    (labels(n2)[0] = 'User' AND n2.userName = e2) OR
                    (labels(n2)[0] = 'Ip' AND n2.ipAddress = e2) OR
                    (labels(n2)[0] = 'Registry' AND n2.keyPath = e2) )
                RETURN e1 AS e1_name, e2 AS e2_name,
                    length(path) AS hops,
                    [n IN nodes(path) | 
                        labels(n)[0] + ':' + coalesce(n.name, n.processName, n.filePath, n.userName, n.ipAddress, n.keyPath, '') 
                    ] AS path_nodes
                LIMIT 50

            """
            indirect_connections_result = session.run(query, trace_ids=similar_ids)
            indirect_connections_raw = [r.data() for r in indirect_connections_result]

            # ì¤‘ë³µ ì œê±° (ì–‘ë°©í–¥ ì—°ê²° ê³ ë ¤)
            seen_connections = set()
            indirect_connections = []

            for conn in indirect_connections_raw:
                e1_name = conn["e1_name"]
                e2_name = conn["e2_name"]
                connection_key = tuple(sorted([e1_name, e2_name]))

                if connection_key not in seen_connections:
                    seen_connections.add(connection_key)
                    indirect_connections.append(conn)

        #     # ëŒ€ì‘ ì œì•ˆ ìƒì„±
        mitigation_prompt = generate_mitigation_prompt(
            summary_result, comparisons, indirect_connections
        )
        mitigation_response = llm.invoke(mitigation_prompt)

        return {
            "summary": summary_result,
            "semantic_top_traces": top_similar_traces,
            "structural_similarity": comparisons,
            "indirect_connections": indirect_connections,
            "mitigation_suggestions": mitigation_response.content,
        }


#     trace_path = "C:\\Users\\KISIA\\Downloads\\data\\T1018.json"
